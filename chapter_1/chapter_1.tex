\documentclass[a4paper]{book}
\special{dvipdfmx:config z 0} %取消PDF压缩，加快速度，最终版本生成的时候最好把这句话注释掉

\usepackage{amssymb}
\usepackage{geometry}
\geometry{
	left=2cm,
	right=2cm,
	top=2cm,
	bottom=2cm,
}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,            %链接颜色
    linkcolor=blue,             %内部链接
    filecolor=magenta,          %本地文档
    urlcolor=cyan,              %网址链接
}
\usepackage[none]{hyphenat}		% 阻止长单词分在两行
\usepackage{mathrsfs}
\usepackage[version=4]{mhchem}
\usepackage{subcaption}
\usepackage{titlesec}

\RequirePackage[many]{tcolorbox}
\tcbset{
    boxed title style={colback=magenta},
	breakable,
	enhanced,
	sharp corners,
	attach boxed title to top left={yshift=-\tcboxedtitleheight,  yshifttext=-.75\baselineskip},
	boxed title style={boxsep=1pt,sharp corners},
    fonttitle=\bfseries\sffamily,
}

\definecolor{skyblue}{rgb}{0.54, 0.81, 0.94}

\newcounter{exercise}[chapter]
\newcounter{solution}[chapter]
\newcounter{eqs}[solution]

\newenvironment{sequation}
  {\begin{equation}\stepcounter{eqs}\tag{\thesolution-\theeqs}}
  {\end{equation}}

\newtcolorbox[use counter=exercise, number within=chapter, number format=\arabic]{exercise}[1][]{
    title={Exercise~\thetcbcounter},
    colframe=skyblue,
    colback=skyblue!12!white,
    boxed title style={colback=skyblue},
    overlay unbroken and first={
        \node[below right,font=\small,color=skyblue,text width=.8\linewidth]
        at (title.north east) {#1};
    }
}

\newtcolorbox[use counter=solution, number within=chapter, number format=\arabic]{solution}[1][]{
    title={Solution~\thetcbcounter},
    colframe=teal!60!green,
    colback=green!12!white,
    boxed title style={colback=teal!60!green},
    overlay unbroken and first={
        \node[below right,font=\small,color=red,text width=.8\linewidth]
        at (title.north east) {#1};
    }
}

% special new commands for common symbols used in the article
\newcommand\tr[1]{\mathrm{tr(#1)}}
\newcommand*{\dif}{\mathop{}\!\mathrm{d}}
\renewcommand\det[1]{\mathrm{det\left(#1\right)}}

\newcommand{\A}{{\bf A}}
\newcommand{\B}{{\bf B}}
\newcommand{\C}{{\bf C}}
\newcommand{\I}{{\bf 1}}
\newcommand{\U}{{\bf U}}
\newcommand{\Op}{{\bf O}}

\titleformat{\chapter}[display]
  {\bfseries\Large}
  {\filright\MakeUppercase{\chaptertitlename} \Huge\thechapter}
  {1ex}
  {\titlerule\vspace{1ex}\filleft}
  [\vspace{1ex}\titlerule]
  
\allowdisplaybreaks

\begin{document}

	\chapter{Mathematical Review}
	
	\section{Linear Algebra}
	
	\subsection{Three-Dimensional Vector Algebra}
	
	% 1.1
	\begin{exercise}
	a) Show that $O_{ij} = \hat e_i \cdot \mathscr{O} \hat{e}_j$. b) If $\mathscr{O} \vec{a} = \vec{b}$ show that $b_i = \displaystyle \sum_j O_{ij} a_j$.
	\end{exercise}
	
	\begin{solution}
	
	\begin{enumerate}
	
	\item Using (1.7) and (1.13), we get that
	\begin{sequation}
		\hat{e}_i \cdot \mathscr{O} \hat{e}_j = \hat{e}_i \cdot \sum_{k=1}^3 \hat{e}_k O_{kj} = \sum_{k=1}^3 \hat{e}_i \cdot  \hat{e}_k O_{kj} = \sum_{k=1}^3 \delta_{ik} O_{kj} = O_{ij}.
	\end{sequation}
	
	\item Similarly,
	\[
		\vec{b} = \sum_{i=1}^3 b_i\hat{e}_i = \mathscr{O} \vec{a} = \mathscr{O} \sum_{j=1}^3 a_j \hat{e}_j = \sum_{j=1}^3 a_j \mathscr{O} \hat{e}_j = \sum_{j=1}^3 a_j \sum_{i=1}^3 \hat{e}_i O_{ij} = \sum_{i=1}^3 \Big( \sum_{j=1}^3 O_{ij} a_j \Big) \hat{e}_i.
	\]
	From the uniqueness of linear expression by a basis, we arrive at
	\begin{sequation}
		b_i = \sum_{j=1}^3 O_{ij} a_j.
	\end{sequation}
	% These two conclusions have been proved.
	
	\end{enumerate}
	
	\end{solution}

	% 1.2
	\begin{exercise}
	Calculate $[\A,\B]$ and $\{\A,\B\}$ when
	\[
		\A = \begin{pmatrix}
					1 & 1 & 0 \\
					1 & 2 & 2 \\
					0 & 2 & -1
		\end{pmatrix}, \quad \B = \begin{pmatrix}
					1 & -1 & 1 \\
					-1 & 0 & 0 \\
					1 & 0 & 1
		\end{pmatrix}.
	\]
	\end{exercise}

	\begin{solution}
	
	\begin{align*}
		[\A,\B] &\equiv \A\B-\B\A = \begin{pmatrix}
		0 & -1 & 1 \\
		1 & -1 & 3 \\
		-3 & 0 & -1
		\end{pmatrix} - \begin{pmatrix}
		0 & 1 & -3 \\
		-1 & -1 & 0 \\
		1 & 3 & -1
		\end{pmatrix} = \begin{pmatrix}
					0 & -2 & 4 \\
					2 & 0 & 3 \\
					-4 & -3 & 0
		\end{pmatrix}, \\
		\{\A,\B\} &\equiv \A\B + \B\A = \begin{pmatrix}
		0 & -1 & 1 \\
		1 & -1 & 3 \\
		-3 & 0 & -1
		\end{pmatrix} + \begin{pmatrix}
		0 & 1 & -3 \\
		-1 & -1 & 0 \\
		1 & 3 & -1
		\end{pmatrix} = 
		\begin{pmatrix}
					0	&	0	&	-2	\\
					0	&	-2	&	3	\\
					-2	&	3	&	-2
		\end{pmatrix}.
	\end{align*}
	
	\end{solution}
	
	\subsection{Matrices}
	
	% 1.3
	\begin{exercise}
		If $\A$ is an $N \times M$ matrix and $\B$ is a $M \times K$ matrix show that $(\A\B)^\dagger = \B^\dagger \A^\dagger$.
	\end{exercise}
	
	\begin{solution}
	
	It is obvious that
	\begin{sequation}
		(\B^\dagger \A^\dagger)_{ij} = \sum_{k=1}^M (\B^\dagger)_{ik} (\A^\dagger)_{kj} = \sum_{k=1}^M B_{ki}^* A^*_{jk} = \left(\sum_{k=1}^M A_{jk} B_{ki} \right)^* = [(\A\B)^*]_{ji} = [(\A\B)^\dagger]_{ij} ,
	\end{sequation}
	which means that $(\A\B)^\dagger = \B^\dagger \A^\dagger$.
	
	\end{solution}
	
	% 1.4
	\begin{exercise}
	Show that 
	\begin{enumerate}
	
	\item[a.] $\tr{\A\B} = \tr{\B\A}$.
	
	\item[b.] $(\A\B)^{-1}=\B^{-1}\A^{-1}$.
	
	\item[c.] If $\U$ is unitary and $\B = \U^\dagger \A \U$, then $\A = \U \B \U^\dagger$.
	
	\item[d.] If the product $\C=\A\B$ of two Hermitian matrices is also Hermitian, then $\A$ and $\B$ commute.
	
	\item[e.] If $\A$ is Hermitian then $\A^{-1}$, if it exists, is also Hermitian.
	
	\item[f.] If $\A=\begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22}	\end{pmatrix}$, then $\A^{-1}=\frac{1}{ ( A_{11} A_{22} - A_{12} A_{21} ) }\begin{pmatrix} A_{22} & -A_{12} \\ -A_{21} & A_{11}	\end{pmatrix}$.
	
	\end{enumerate}
	\end{exercise}
	
	\begin{solution}
	\begin{enumerate}
	
	\item[a.] At this time, we assume that $\A$ is an $N \times M$ matrix while $\B$ is a $M \times N$ matrix. Then,
	\begin{sequation}
		\tr{\A\B} = \sum_{i=1}^N (\A\B)_{ii} = \sum_{i=1}^N \sum_{k=1}^M A_{ik} B_{ki} = \sum_{k=1}^M \sum_{i=1}^N B_{ki} A_{ik} = \sum_{k=1}^M (\B\A)_{kk} = \tr{\B\A}.
	\end{sequation}
	
	From this issue, we assume that both $\A$ and $\B$ are $N \times N$ matrices.
	
	\item[b.] We find that
	\[
		\A\B (\B^{-1}\A^{-1}) = \A(\B \B^{-1})\A^{-1} = \A \A^{-1} = \I.
	\]
	Since the inverse of a matrix is unique, we immediately get that
	\begin{sequation}
		(\A\B)^{-1}=\B^{-1}\A^{-1}.
	\end{sequation}
	
	\item[c.] Due to $\B = \U^\dagger \A \U$, we can find
	\begin{sequation}
		\A = \I \A \I = (\U \U^\dagger) \A (\U \U^\dagger) = \U (\U^\dagger \A \U )\U^\dagger = \U \B \U^\dagger.
	\end{sequation}
	
	\item[d.] Because $\C=\A\B$ of two Hermitian matrices is also Hermitian, we know that
	\[
		\C^\dagger = (\A\B)^\dagger = \B^\dagger \A^\dagger = \C = \A \B. 
	\]
	With $\A^\dagger = \A, \, \B^\dagger = \B$, we find
	\begin{sequation}
		\B^\dagger \A^\dagger = \B \A = \A \B.
	\end{sequation}
	In other words, $\A$ and $\B$ commute.
	
	\item[e.] It is obvious that if $\A^{-1}$ exists,
	\[
		(\A^{-1})^\dagger \A^\dagger = (\A \A^{-1})^\dagger = \I^\dagger = \I.
	\]
	We know $(\A^\dagger)^{-1} = (\A^{-1})^\dagger$. Then, with $\A = \A^\dagger$, we find that
	\begin{sequation}
		(\A^{-1})^\dagger = (\A^\dagger)^{-1} = \A^{-1}.
	\end{sequation}
	Namely, $\A^{-1}$ is also Hermitian if it exists.
	
	\item[f.] If $A_{11}A_{22}-A_{12}A_{21}\neq 0$, we can find
	\[
		\A \times \frac{1}{A_{11}A_{22}-A_{12}A_{21}}\begin{pmatrix} A_{22} & -A_{12} \\ -A_{21} & A_{11}	\end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix},
	\]
	which means that if $A_{11}A_{22}-A_{12}A_{21} \neq 0$,
	\begin{sequation}
		\A^{-1} = \frac{1}{ A_{11} A_{22} - A_{12} A_{21} }\begin{pmatrix} A_{22} & -A_{12} \\ -A_{21} & A_{11}	\end{pmatrix}.
	\end{sequation}
		
	\end{enumerate}
	
	\end{solution}
	
	\subsection{Determinants}
	
	% 1.5
	\begin{exercise}
	Verify the above properties for $2\times2$ determinants.
	\end{exercise}
	
	\begin{solution}

	From (1.39), we can verify the above properties for $2\times2$ determinants.

	\begin{itemize}
	
	\item[1.] If each element in the first row of $|\A|$ is zero, we will find that
	\[
		\begin{vmatrix}
			0  & 0 \\ A_{21} &  A_{22} 
		\end{vmatrix} = 0 \times A_{22} - 0 \times A_{21} = 0 .
	\]
	In the same way, we can verify that if each element in a row or in a column is zero, the value of the $2 \times 2$ determinant $|\A|$ is zero.
	
	\item[2.] If $(\A)_{ij} = A_{ii} \delta_{ij}$, we will find that
	\[
		\begin{vmatrix}
			A_{11}  & 0 \\ 0 & A_{22} 
		\end{vmatrix} = A_{11} A_{22} - 0 \times 0 = A_{11} A_{22} = \prod_{ i=1 }^2 A_{ii} .
	\]
	
	\item[3.] We can interchange two rows of the $2\times2$ determinant $|\A|$ and find that
	\[
		\begin{vmatrix}
			A_{21}  & A_{22} \\ A_{11} & A_{12} 
		\end{vmatrix} = A_{21} A_{12} - A_{11} A_{22} = - ( A_{11} A_{22} - A_{12} A_{21} ) = - \begin{vmatrix}
			A_{11}  & A_{12} \\ A_{21} & A_{22} 
		\end{vmatrix} .
	\]
	In the same way, we can verify that a single interchange of any two rows (or columns) of a determinant changes its sign.
	
	\item[4.] Note that $(\A^\dagger)_{ij} = A^*_{ji}$,
	\[
		(| \A^\dagger |)^* = \begin{vmatrix}
			A^*_{11}  & A^*_{21} \\ A^*_{12} & A^*_{22} 
		\end{vmatrix}^* = ( A^*_{11} A^*_{22} - A^*_{12} A^*_{21} )^* = A_{11} A_{22} - A_{12} A_{21} = | \A | .
	\]	
	
	\item[5.] It is evident that for two $2\times2$ matrices $\A$ and $\B$, the determinant of their product is
	\[
		\A \B = \begin{pmatrix}
			A_{11} & A_{12} \\ A_{21} & A_{22} 
		\end{pmatrix} \begin{pmatrix}
			B_{11} & B_{12} \\ B_{21} & B_{22} 
		\end{pmatrix} = \begin{pmatrix}
			A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\
			A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}
		\end{pmatrix} .
	\]
	Thus, we can find that
	\begin{align*}
		\det{\A \B} &= \begin{vmatrix}
			A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\
			A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}
		\end{vmatrix} \\
		&= ( A_{11} B_{11} + A_{12} B_{21} ) ( A_{21} B_{12} + A_{22} B_{22} ) - ( A_{11} B_{12} + A_{12} B_{22} ) ( A_{21} B_{11} + A_{22} B_{21} ) \\
		&= A_{11} A_{21} B_{11} B_{12} + A_{11} A_{22} B_{11} B_{22} + A_{12} A_{21} B_{12} B_{21} + A_{12} A_{22} B_{21} B_{22} \\
		&\hspace{4em} - A_{11} A_{21} B_{11} B_{12} - A_{11} A_{22} B_{12} B_{21} - A_{12} A_{21} B_{11} B_{22} - A_{12} A_{22} B_{21} B_{22} \\
		&= A_{11} A_{22} ( B_{11} B_{22} - B_{12} B_{21} ) + A_{12} A_{21} ( B_{12} B_{21} - B_{11} B_{22} ) \\
		&= ( A_{11} A_{22} - A_{12} A_{21} ) ( B_{11} B_{22} - B_{12} B_{21} ) = | \A | | \B | .
	\end{align*}
	
	\end{itemize}		
	
	\end{solution}
	
	% 1.6
	\begin{exercise}
	Using properties (1)-(5) prove that in general
	\begin{itemize}
	
	\item[6.] If any two rows (or columns) of a determinant are equal, the value of the determinant is zero.
	
	\item[7.] $|\A^{-1}| = (|\A|)^{-1}$.
	
	\item[8.] If $\A\A^\dagger=\I$, then $|\A|(|\A|)^*=1$.
	
	\item[9.] If $\U^\dagger\Op\U = {\bf \Omega}$ and $\U^\dagger\U=\U\U^\dagger=\I$, then $|\Op|=|{\bf \Omega}|$.	
	
	\end{itemize}
	\end{exercise}
	
	\begin{solution}
	
	\begin{itemize}
	
	\item[6.] According to the property 3, a single interchange of any two rows (or columns) of a determinant $|\A|$ generates a minus sign. However, a single interchange of any two equal rows (or columns) does not change $|\A|$, which means $|\A| = -|\A|$ and thus $|\A| = 0$.
	
	\item[7.] For a general square matrix $\A$, it is evident that
	\[
		1 = |\I| = |\A \A^{-1}| = |\A| |\A^{-1}| .
	\] 
	Therefore,
	\[
		| \A^{-1} | = ( |\A| )^{-1} .
	\]
	
	\item[8.] Using the property 4, we find that
	\[
		1 = |\I| = | \A \A^\dagger | = | \A | | \A^\dagger | = | \A | (| \A |)^* .
	\]
	
	\item[9.] Note that	
	\[
		\U \U^\dagger = \I \Leftrightarrow \U^{-1} = \U^\dagger \Rightarrow | \U^{-1} | = | \U^\dagger | = | \U |^* ,
	\]	
	and thus $1 = | \I | = | \U^\dagger \U | = | \U^\dagger | | \U | = | \U |^* | \U |$. Therefore, we obtain that
	\[
		| {\bf \Omega} | = | \U^\dagger \Op \U | = | \U^\dagger | | \Op \U | = | \U |^* | \Op | | \U | = | \Op | | \U |^* | \U | = | \Op | .
	\]
	
	\end{itemize}		
	
	\end{solution}
	
	% 1.7
	\begin{exercise}
	Using Eq.(1.39), note that the inverse of a $2\times2$ matrix $\A$ obtained in Exercise 1.4f can be written as
	\[
		\A^{-1} = \frac{1}{|\A|}\begin{pmatrix}
			A_{22} & -A_{12} \\ -A_{21} & A_{11}
		\end{pmatrix}
	\]
	and thus $\A^{-1}$ does not exist when $|\A|=0$. This result holds in general for $N\times N$ matrices. Show that the equation
	\[
		\A {\bf c} = {\bf 0}
	\]
	where $\A$ is an $N \times N$ matrix and ${\bf c}$ is a column matrix with elements $c_i$, $i=1,2,\ldots,N$ can have a nontrivial solution (${\bf c} \neq 0$) only when $|\A|=0$.
	\end{exercise}
	
	\begin{solution}
	
	This conclusion is a corollary of Cramer's rule, whose WIKIPEDIA's url is \url{https://en.wikipedia.org/wiki/Cramer's_rule}. Its proof is too lengthy to be omitted here.
	
	\end{solution}
	
	\subsection{\texorpdfstring{$N$}--Dimentional Complex Vector Spaces}
	
	\subsection{Change of Basis}
	
	% 1.8
	\begin{exercise}
	Show that the trace of a matrix is invariant under a unitary transformation, i.e., if ${\bf \Omega} = \U^\dagger \Op \U$ then show that $\tr{{\bf \Omega}}=\tr{\Op}$.
	\end{exercise}
	
	\begin{solution}
	
	Using the conclusion of Exercise 1.4(a), we find that
	\begin{sequation}
		\tr{{\bf \Omega}} = \tr{ \U^\dagger \Op \U } = \tr{ \Op \U \U^\dagger } = \tr{ \Op \I } = \tr{ \Op } .
	\end{sequation}		
	
	\end{solution}
	
	\subsection{The Eigenvalue Problem}
	
	% 1.9
	\begin{exercise}
	Show that Eq.(1.90) contains Eq.(1.87) for all $\alpha=1,2,\ldots,N$.
	\end{exercise}
	
	\begin{solution}
	
	It is evident that
	\begin{sequation}
		\Op \U = \Op ( {\bf c}^1 , {\bf c}^2 , \cdots , {\bf c}^N ) = ( \Op {\bf c}^1 , \Op {\bf c}^2 , \cdots , \Op {\bf c}^N ) = ( \omega_1 {\bf c}^1 , \omega_2 {\bf c}^2 , \cdots , \omega_N {\bf c}^N ) = \U {\bf \omega} ,
	\end{sequation}
	which equals (1.87) for all $\alpha=1,2,\ldots,N$.
	
	\end{solution}
	
	% 1.10
	\begin{exercise}
	Since the components of an eigenvector can be found from the eigenvalue equation only to within a multiplicative constant, which is later determined by the normalization, one can set $c_1 = 1$ and $c_2 = c$ in Eq.(1.94). If this is done, Eq.(1.94) becomes
	\begin{align*}
		O_{11} + O_{12} c &= \omega \\
		O_{21} + O_{22} c &= \omega c. 
	\end{align*}
	After eliminating $c$, find the two roots of the resulting equation and show that they are the same as those given in Eq.(1.96). This technique, which we shall use numerous times in the book for finding the lowest eigenvalue of a matrix, is basically the secular determinant approach {\it without} determinants. Thus one can use it to find the lowest eigenvalue of certain $N \times N$ matrices without having to evaluate an $N \times N$ determinant.
	\end{exercise}
	
	\begin{solution}
	
	The proof should be discussed according to the value of $O_{12} = O_{21}$.
	
	\begin{itemize}
	
	\item When $O_{12} = O_{21} = 0$, assume that $c\neq 0$ and we find that
	\[
		\omega_1 = O_{11} , \quad \omega_2 = O_{22} .
	\]
	If $c=0$, we find that
	\[
		\begin{pmatrix}
			O_{11} & 0 \\ 0 & O_{22} 
		\end{pmatrix} \begin{pmatrix}
			1 \\ 0 
		\end{pmatrix} = \begin{pmatrix}
			Q_{11} \\ 0
		\end{pmatrix} = Q_{11} \begin{pmatrix}
			1 \\ 0
		\end{pmatrix} .
	\]
	At this time, the only $\omega = O_{11}$.
	
	\item When $O_{12} = O_{21} \neq 0$, from the first equation, we get that
	\[
		c = \frac{ \omega - O_{11} }{ O_{12} } .
	\]
	Substitute it into the second equation, we obtain that
	\[
		O_{21} = ( \omega - O_{22} ) c = ( \omega - O_{22} ) \frac{ \omega - O_{11} }{ O_{12} },
	\]
	which equals
	\[
		( \omega - O_{11} )( \omega - O_{22} ) - O_{12} O_{21} = \omega^2 - ( O_{11} + O_{22} ) \omega + ( O_{11} O_{22} - O_{12} O_{21} ) = 0 .
	\]	
	The discriminant of the quadratic equation is
	\[
		\Delta_\omega = ( O_{11} + O_{22} )^2 - 4 \times 1 \times ( O_{11} O_{22} - O_{12} O_{21} ) = ( O_{11} - O_{22} )^2 + 4 O_{12} O_{21} > 0 ,
	\]
	and the two roots are
	\[
		\omega_\pm = \frac{1}{2} \left[ O_{11} + O_{22} \pm \sqrt{ ( O_{11} - O_{22} )^2 + 4 O_{12} O_{21} } \right] .
	\]
	\end{itemize}
	
	Note that
	\begin{align*}
		\lim_{O_{12} \rightarrow 0} \omega_\pm &= \lim_{O_{12} \rightarrow 0} \frac{1}{2} \left[ O_{11} + O_{22} \pm \sqrt{ ( O_{11} - O_{22} )^2 + 4 O_{12} O_{21} } \right] \\
		&= \frac{1}{2} \left( O_{11} + O_{22} \right) \pm \frac{1}{2} \lim_{O_{12} \rightarrow 0} \sqrt{ ( O_{11} - O_{22} )^2 + 4 O_{12} O_{21} } \\
		&= \frac{1}{2} \left( O_{11} + O_{22} \right) \pm \frac{1}{2} | O_{11} - O_{22} | .
	\end{align*}
	
	When $O_{11} \ge O_{22}$,
	\[
		\omega_+ = \frac{1}{2} \left( O_{11} + O_{22} \right) + \frac{1}{2} \left( O_{11} - O_{22} \right) = O_{11} , \quad \omega_- = \frac{1}{2} \left( O_{11} + O_{22} \right) - \frac{1}{2} \left( O_{11} - O_{22} \right) = O_{22} .
	\]	
	while $O_{11} < O_{22}$,
	\[
		\omega_+ = \frac{1}{2} \left( O_{11} + O_{22} \right) + \frac{1}{2} \left( O_{22} - O_{11} \right) = O_{22} , \quad \omega_- = \frac{1}{2} \left( O_{11} + O_{22} \right) - \frac{1}{2} \left( O_{22} - O_{11} \right) = O_{11} .
	\]	
	
	In conclusion, we obtain that
	\[
		\lim_{O_{12} \rightarrow 0} \omega_1 = O_{11} , \quad \lim_{O_{12} \rightarrow 0} \omega_2 = O_{22} .
	\]
	Thus, the special case of $O_{12} = O_{21} = 0$ can be merged in the general case. And we conclude that two roots obtained by eliminating $c$ are the same as those given in Eq.(1.96).
	
	\end{solution}
	
	% 1.11
	\begin{exercise}
	Consider the matrices
	\begin{equation*}
		\A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}, \,  \B = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}.
	\end{equation*}
	Find numerical values for the eigenvalues and corresponding eigenvectors of these matrices by a) the secular determinant approach; b) the unitary transformation approach. You will see that approach (b) is much easier.
	\end{exercise}
	
	\begin{solution}
	
	Using the secular determinant approach, the intermediate results are as follows.
	
	\begin{itemize}
	
	\item For the matrix $\A$. Its eigenpolynomial is
	\begin{align*}
		| \A - \omega \I | = \begin{vmatrix}
			3 - \omega & 1 \\ 1 & 3 - \omega 
		\end{vmatrix} = ( 3 - \omega )^2 - 1 = ( 4 - \omega ) ( 2 - \omega ) = 0.
	\end{align*}
	Its two roots are
	\[
		\omega_1 = 2 , \quad \omega_2 = 4.
	\]
	For the eigenvalue $\omega_1$,
	\[
		\A - \omega_1 \I = \begin{pmatrix}
			3 - 2 & 1 \\ 1 & 3 - 2
		\end{pmatrix} = \begin{pmatrix}
			1 & 1 \\ 1 & 1
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & 1 \\ 0 & 0
		\end{pmatrix} .
	\]
	The eigenvector is $(1,-1)^T$.
	
	For the eigenvalue $\omega_2$,
	\[
		\A - \omega_2 \I = \begin{pmatrix}
			3 - 4 & 1 \\ 1 & 3 - 4
		\end{pmatrix} = \begin{pmatrix}
			-1 & 1 \\ 1 & -1
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & -1 \\ 0 & 0
		\end{pmatrix} .
	\]
	The eigenvector is $(1,1)^T$.
	
	\item For the matrix $\B$. Its eigenpolynomial is
	\begin{align*}
		| \B - \omega \I | = \begin{vmatrix}
			3 - \omega & 1 \\ 1 & 2 - \omega 
		\end{vmatrix} = ( 3 - \omega )( 2 - \omega ) - 1 = \omega^2 - 5 \omega + 5 = 0.
	\end{align*}
	Its two roots are
	\[
		\omega_1 = \frac{ 5 + \sqrt{5} }{2} , \quad \omega_2 = \frac{ 5 - \sqrt{5} }{2} .
	\]
	For the eigenvalue $\omega_1$,
	\[
		\B - \omega_1 \I = \begin{pmatrix}
			3 - \frac{ 5 + \sqrt{5} }{2} & 1 \\ 1 & 2 - \frac{ 5 + \sqrt{5} }{2}
		\end{pmatrix} = \begin{pmatrix}
			\frac{ 1 - \sqrt{5} }{2} & 1 \\ 1 & \frac{ - 1 - \sqrt{5} }{2}
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & - \frac{ 1 + \sqrt{5} }{2} \\ 0 & 0
		\end{pmatrix} .
	\]
	The eigenvector is $( \frac{ 1 + \sqrt{5} }{2} , 1 )^T$.
	
	For the eigenvalue $\omega_2$,
	\[
		\B - \omega_2 \I = \begin{pmatrix}
			3 - \frac{ 5 - \sqrt{5} }{2} & 1 \\ 1 & 2 - \frac{ 5 - \sqrt{5} }{2}
		\end{pmatrix} = \begin{pmatrix}
			\frac{ 1 + \sqrt{5} }{2} & 1 \\ 1 & \frac{ -1 + \sqrt{5} }{2}
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & \frac{ -1 + \sqrt{5} }{2} \\ 0 & 0
		\end{pmatrix} .
	\]
	The eigenvector is $( \frac{ 1 - \sqrt{5} }{2} , 1 )^T$.
	
	\end{itemize}
	
	Using the unitary transformation approach, the intermediate results are as follows.
	\begin{itemize}
	
	\item For the matrix $\A$. Its $\theta_0$ is
	\[
		\theta_0 = \frac{1}{2} \arctan{\frac{ 2O_{12} }{ O_{11} - O_{22} } } = \frac{1}{2} \arctan{ \frac{ 2 \times 1 }{ 3 - 3 } } = \frac{1}{2} \arctan{ \infty } = \frac{1}{2} \times \frac{ \pi }{ 2 } = \frac{ \pi }{ 4 } .
	\]
	Here note that $\infty$	means that both $+\infty$ and $-\infty$ are allowed but only $+\infty$ is used for the following calculation. The same results can be obtained by using $-\infty$ and this process is omitted for the sake of simplicity!
	
	From (1.106a,b) and (1.107a,b), we obtain that the first eigenvalue $\omega_1$ is
	\[
		\omega_1 = O_{11} \cos^2 \theta_0 + O_{22} \sin^2 \theta_0 + O_{12} \sin 2\theta = 3 \cos^2 \frac{\pi}{4} + 3 \sin^2 \frac{\pi}{4} + 1 \sin \left( \frac{2 \pi}{4} \right) = 4 ,
	\]
	whose eigenvector is
	\[
		\begin{pmatrix}
			c^1_1 \\ c^1_2 
		\end{pmatrix} = \begin{pmatrix}
			\cos \frac{\pi}{4} \\ \sin \frac{\pi}{4}
		\end{pmatrix} = \begin{pmatrix}
			\frac{1}{ \sqrt{2} } \\ \frac{1}{ \sqrt{2} }
		\end{pmatrix} ,
	\]
	and the second eigenvalue $\omega_2$ is
	\[
		\omega_2 = O_{11} \sin^2 \theta_0 + O_{22} \cos^2 \theta_0 - O_{12} \sin 2\theta = 3 \sin^2 \frac{\pi}{4} + 3 \cos^2 \frac{\pi}{4} - 1 \sin \left( \frac{2 \pi}{4} \right) = 2 ,
	\]
	whose eigenvector is
	\[
		\begin{pmatrix}
			c^2_1 \\ c^2_2 
		\end{pmatrix} = \begin{pmatrix}
			\sin \frac{\pi}{4} \\ -\cos \frac{\pi}{4}
		\end{pmatrix} = \begin{pmatrix}
			\frac{1}{ \sqrt{2} } \\ -\frac{1}{ \sqrt{2} }
		\end{pmatrix} .
	\]
	
	\item For the matrix $\B$. Its $\theta_0$ is
	\[
		\theta_0 = \frac{1}{2} \arctan{\frac{ 2O_{12} }{ O_{11} - O_{22} } } = \frac{1}{2} \arctan{ \frac{ 2 \times 1 }{ 3 - 2 } } = \frac{1}{2} \arctan{ 2 } .
	\]
	In other words, 
	\[
		2 = \tan 2\theta_0 = \frac{ 2 \tan \theta_0 }{ 1 - \tan^2 \theta_0 } \Rightarrow \tan (\theta_0)_\pm = \frac{ -1 \pm \sqrt{5} }{2}. 
	\]
	We use $\tan \theta_0 = \frac{ -1 + \sqrt{5} }{2}$ in the following calculation. Of course, with $\tan \theta_0 = \frac{ -1 - \sqrt{5} }{2}$, the same results can be omitted and the derivation is omitted, too.
	
	Thus, we know that from
	\begin{align*}
		\sin^2 \theta_0 + \cos^2 \theta_0 &= 1 , \\
		\frac{ \sin \theta_0 }{ \cos \theta_0 } = \tan \theta_0 &= \frac{ -1 + \sqrt{5} }{2},
	\end{align*}
	we get
	\begin{align*}
		\cos^2 \theta_0 &= \frac{1}{ 1 + \left( \frac{ -1 + \sqrt{5} }{2} \right)^2 } = \frac{ 4 }{ 4 + 6 - 2\sqrt{5} } = \frac{ 4 }{ 2\sqrt{5} ( \sqrt{5} - 1 ) } = \frac{ \sqrt{5} + 1 }{ 2\sqrt{5} } , \\
		\sin^2 \theta_0 &= 1 - \cos^2 \theta_0 = 1 - \frac{ \sqrt{5} + 1 }{ 2\sqrt{5} } = \frac{ -1 + \sqrt{5} }{ 2\sqrt{5} } , \\
		\sin 2\theta_0 &= 2 \sin \theta_0 \cos \theta_0 = 2 \frac{ -1 + \sqrt{5} }{ 2 } \cos \theta_0 \cos \theta_0 = ( -1 + \sqrt{5} ) \cos^2 \theta_0 = \frac{ 2 }{ \sqrt{5} } .
	\end{align*}
	
	From (1.106a,b) and (1.107a,b), we obtain that the first eigenvalue $\omega_1$ is
	\[
		\omega_1 = O_{11} \cos^2 \theta_0 + O_{22} \sin^2 \theta_0 + O_{12} \sin 2\theta = 3 \frac{ \sqrt{5} + 1 }{ 2\sqrt{5} } + 2 \frac{ -1 + \sqrt{5} }{ 2\sqrt{5} } + 1 \frac{ 2 }{ \sqrt{5} } = \frac{ 5 + \sqrt{5} }{2} .
	\]
	whose eigenvector is
	\[
		\begin{pmatrix}
			c^1_1 \\ c^1_2 
		\end{pmatrix} = \begin{pmatrix}
			\cos \theta_0 \\ \sin \theta_0
		\end{pmatrix} \Leftrightarrow \begin{pmatrix}
			\cos^2 \theta_0 \\ \sin \theta \cos \theta_0
		\end{pmatrix} = \begin{pmatrix}
			\cos^2 \theta_0 \\ \frac{1}{2}\sin 2\theta_0
		\end{pmatrix} = \begin{pmatrix}
			\frac{ \sqrt{5} + 1 }{ 2\sqrt{5} } \\ \frac{ 1 }{ \sqrt{5} }
		\end{pmatrix} \Leftrightarrow \begin{pmatrix}
			\frac{ \sqrt{5} + 1 }{ 2 } \\ 1
		\end{pmatrix} .
	\]
	and the second eigenvalue $\omega_2$ is
	\[
		\omega_2 = O_{11} \sin^2 \theta_0 + O_{22} \sin^2 \theta_0 - O_{12} \sin 2\theta = 3 \frac{ -1 + \sqrt{5} }{ 2\sqrt{5} } + 2 \frac{ \sqrt{5} + 1 }{ 2\sqrt{5} } - 1 \frac{ 2 }{ \sqrt{5} } = \frac{ 5 - \sqrt{5} }{ 2 } .
	\]
	whose eigenvector is
	\[
		\begin{pmatrix}
			c^2_1 \\ c^2_2 
		\end{pmatrix} = \begin{pmatrix}
			\sin \theta_0 \\ -\cos \theta_0
		\end{pmatrix} = \begin{pmatrix}
			\frac{1}{2} \sin 2\theta_0 \\ -\cos^2 \theta_0
		\end{pmatrix} \Leftrightarrow  \begin{pmatrix}
			1 \\ \frac{ -\sqrt{5} - 1 }{ 2 }
		\end{pmatrix} = \begin{pmatrix}
			\frac{ 1 - \sqrt{5} }{ 2 } \\ 1 
		\end{pmatrix} .
	\]
	
	\end{itemize}
	
	I think that the first approach is much easier.
	
	\end{solution}	

	\subsection{Functions of Matrices}
	
	% 1.12
	\begin{exercise}
	Given that
	\begin{equation*}
		\U^\dagger \A \U = {\bf a} = \begin{pmatrix} a_1 & 0 & \cdots & 0 \\ 0 & a_2 & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & a_N \end{pmatrix} \quad \text{or} \quad \A {\bf c^\alpha} = a_\alpha {\bf c^\alpha} , \, \alpha = 1,2,\cdots, N.
	\end{equation*}
	Show that
	\begin{enumerate}
	
	\item[a.] $\det{\A^n}=a^n_1 a^n_2 \cdots a^n_N$.

	\item[b.] $\tr{\A^n}=\displaystyle\sum_{\alpha=1}^N a^n_\alpha$.
	
	\item[c.] If ${\bf G}(\omega) = (\omega\I - \A)^{-1}$, then
	\begin{equation*}
		({\bf G}(\omega))_{ij} = \sum_{\alpha=1}^N \frac{U_{i\alpha}U^*_{j\alpha}}{\omega-a_\alpha} = \sum_{\alpha=1}^N \frac{c^\alpha_i {c^\alpha_j}^*}{\omega-a_\alpha}.
	\end{equation*}
	Show that using Dirac notation this can be rewritten as
	\begin{equation*}
		({\bf G}(\omega))_{ij} \equiv \langle i | \mathscr{G}(\omega) | j \rangle = \sum_{\alpha} \frac{\langle i | \alpha \rangle \langle \alpha | j \rangle }{\omega - a_\alpha}.
	\end{equation*}
	\end{enumerate}
	
	\end{exercise}
	
	\begin{solution}
	
	Before the formal derivation, note that
	\[
		\U^\dagger \A \U = {\bf a} \Leftrightarrow \A = \U {\bf a} \U^\dagger \Rightarrow \A^n = (  \U {\bf a} \U^\dagger )^n = \U {\bf a}^n \U^\dagger .
	\]	
	
	\begin{itemize}
	
	\item[a.] Using $\det{\A\B} = \det{\A} \det{\B} = \det{\B} \det{\A}$ and $\det{\I} = 1$, we find that
	\begin{align*}
		\det{ \A^n } &= \det{ \U {\bf a}^n \U^\dagger } = \det{ \U } \det{ {\bf a}^n \U^\dagger } = \det{ {\bf a}^n \U^\dagger } \det{ \U } \\
		&= \det{ {\bf a}^n  } \det{ \U^\dagger } \det{ \U } = \det{ {\bf a}^n  } \det{ \U^\dagger \U } = \det{ {\bf a}^n } \det{ \I } = \det{ {\bf a}^n } .
	\end{align*}
	
	\item[b.] Using $\tr{\A\B} = \tr{\B\A}$, we find that
	\begin{align*}
		\tr{\A^n} = \tr{ \U {\bf a}^n \U^\dagger } = \tr{ {\bf a}^n \U^\dagger \U } = \tr{{\bf a}^n} .
	\end{align*}
	
	\item[c.] From $\A = \U {\bf a} \U^\dagger$, we get that
	\[
		\omega \I - \A = \omega \U \I \U^\dagger - \U {\bf a} \U^\dagger = \U ( \omega \I - {\bf a} ) \U^\dagger .
	\]
	Thus,
	\[
		{\bf G}(\omega) = (\omega\I - \A)^{-1} = [ \U ( \omega \I - {\bf a} ) \U^\dagger ]^{-1} = ( \U^\dagger )^{-1} ( \omega \I - {\bf a} )^{-1} \U^{-1} = \U ( \omega \I - {\bf a} )^{-1} \U^\dagger ,
	\]
	and then
	\begin{sequation}
		({\bf G}(\omega))_{ij} = [ \U ( \omega \I - {\bf a} )^{-1} \U^\dagger ]_{ij} = \sum_{\alpha=1}^N \sum_{\beta=1}^N (\U)_{i\alpha} \frac{ \delta_{\alpha \beta} }{ \omega - a_\alpha  } (\U^\dagger)_{\beta j} = \sum_{\alpha=1}^N \frac{ U_{i\alpha} U^*_{j\alpha} }{ \omega - a_\alpha } .
	\end{sequation}
	
	Note that in the basis $\{ | \alpha \rangle \}$, $\U$ is diagonal, viz., $\U = \sum_\beta | \beta \rangle \langle \beta | $ and thus $\U | \alpha \rangle = | \alpha \rangle$. Using Dirac notation, the final result equals 
	\begin{align*}
		({\bf G}(\omega))_{ij} &= \langle i | \U ( \omega \I - {\bf a} )^{-1} \U^\dagger | j \rangle = \sum_\alpha \sum_\beta \langle i | \U | \alpha \rangle \langle \alpha | \frac{1}{ \omega \I - {\bf a} } | \beta \rangle \langle \beta | \U^\dagger | j \rangle \\
		&= \sum_\alpha \sum_\beta \langle i | \U | \alpha \rangle \frac{ \delta_{\alpha \beta} }{ \omega - a_\alpha } \langle \beta | \U^\dagger | j \rangle = \sum_\alpha \frac{ \langle i | \U | \alpha \rangle \langle \alpha | \U^\dagger | j \rangle }{ \omega - a_\alpha } = \sum_\alpha \frac{ \langle i | \alpha \rangle \langle \alpha | j \rangle }{ \omega - a_\alpha } .
	\end{align*}
	
	\end{itemize}
	
	\end{solution}
	
	% 1.13
	\begin{exercise}
	If
	\begin{equation*}
		\A = \begin{pmatrix} a & b \\ b & a \end{pmatrix}			
	\end{equation*}
	show that
	\begin{equation*}
		f(\A) = \begin{pmatrix}
		\frac{1}{2}[f(a+b)+f(a-b)] & \frac{1}{2}[f(a+b)-f(a-b)] \\
		\frac{1}{2}[f(a+b)-f(a-b)] & \frac{1}{2}[f(a+b)+f(a-b)]
		\end{pmatrix}.
	\end{equation*}
	\end{exercise}
	
	\begin{solution}
	
	The eigenpolynomial of $\A$ is
	\[
		\det{\A-\varepsilon\I} = \begin{vmatrix}
			a - \varepsilon & b \\ b & a - \varepsilon
		\end{vmatrix} = ( a - \varepsilon )^2 - b^2 = ( a + b - \varepsilon ) ( a - b - \varepsilon ) = 0.
	\]
	It has two roots,
	\[
		\varepsilon_1 = a - b , \quad \varepsilon_2 = a + b.
	\]
	The eigenvector of $\varepsilon_1$ is
	\[
		\A - \varepsilon_1 \I = \begin{pmatrix}
			a - ( a - b ) & b \\ b & a - ( a - b )
		\end{pmatrix} = \begin{pmatrix}
			b & b \\ b & b 
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & 1 \\ 0 & 0 
		\end{pmatrix}.
	\]
	The normalized eigenvector is $\frac{1}{ \sqrt{2} }(1,-1)^T$.
	
	Besides, the eigenvector of $\varepsilon_2$ is
	\[
		\A - \varepsilon_2 \I = \begin{pmatrix}
			a - ( a + b ) & b \\ b & a - ( a + b )
		\end{pmatrix} = \begin{pmatrix}
			-b & b \\ b & -b 
		\end{pmatrix} \Rightarrow \begin{pmatrix}
			1 & -1 \\ 0 & 0 
		\end{pmatrix}.
	\]
	The normalized eigenvector is $\frac{1}{ \sqrt{2} }(1,1)^T$.
	
	Thus, we know that
	\[
		\A = \U {\bf a} \U^\dagger = \begin{pmatrix}
			\frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{2} } \\
			\frac{1}{ \sqrt{2} } & -\frac{1}{ \sqrt{2} }
		\end{pmatrix} \begin{pmatrix}
			a + b & 0 \\ 0 & a - b 
		\end{pmatrix} \begin{pmatrix}
			\frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{2} } \\
			\frac{1}{ \sqrt{2} } & -\frac{1}{ \sqrt{2} }
		\end{pmatrix} .
	\] 
	and thus
	\begin{align*}
		f(\A) = \U f( {\bf a} ) \U^\dagger &= \begin{pmatrix}
			\frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{2} } \\
			\frac{1}{ \sqrt{2} } & -\frac{1}{ \sqrt{2} }
		\end{pmatrix} \begin{pmatrix}
			f( a + b ) & 0 \\ 0 & f( a - b )
 		\end{pmatrix} \begin{pmatrix}
			\frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{2} } \\
			\frac{1}{ \sqrt{2} } & -\frac{1}{ \sqrt{2} }
		\end{pmatrix} \\
		&= \begin{pmatrix}
			\frac{1}{ \sqrt{2} } f( a + b ) & \frac{1}{ \sqrt{2} } f( a - b ) \\
			\frac{1}{ \sqrt{2} } f( a + b ) & -\frac{1}{ \sqrt{2} } f( a - b ) 
		\end{pmatrix} \begin{pmatrix}
			\frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{2} } \\
			\frac{1}{ \sqrt{2} } & -\frac{1}{ \sqrt{2} }
		\end{pmatrix} \\
		&= \begin{pmatrix}
		\frac{1}{2}[f(a+b)+f(a-b)] & \frac{1}{2}[f(a+b)-f(a-b)] \\
		\frac{1}{2}[f(a+b)-f(a-b)] & \frac{1}{2}[f(a+b)+f(a-b)]
		\end{pmatrix} .
	\end{align*}
	
	\end{solution}
	
	\section{Orthogonal Functions, Eigenfunctions, and Operators}
	
	% 1.14
	\begin{exercise}
	Using the above representation of $\delta(x)$, show that
	\begin{equation*}
		a(0) = \int_{-\infty}^\infty \dif x \, a(x) \delta(x) .
	\end{equation*}
	\end{exercise}
	
	\begin{solution}
	
	Using first mean value theorem for definite integrals, we find that
	\begin{align*}
		\int_{-\infty}^\infty \dif x \, a(x) \delta(x) &= \int_{-\infty}^\infty \dif x \, a(x) \lim_{\varepsilon \rightarrow 0}  \delta_\varepsilon(x) = \lim_{\varepsilon \rightarrow 0} \int_{-\varepsilon}^\varepsilon \dif x \, a(x) \delta_\varepsilon(x) \\
		&= \lim_{\varepsilon \rightarrow 0} \frac{ 1 }{ 2 \varepsilon } \int_{-\varepsilon}^\varepsilon \dif x \, a(x) = \lim_{ \substack{ \varepsilon \rightarrow 0 \\ \zeta \in ( -\varepsilon, \varepsilon )} } \frac{ 1 }{ 2 \varepsilon }a(\zeta) [ \varepsilon - ( - \varepsilon ) ]  = a(0) .
	\end{align*}		
	
	More detailed description about first mean value theorem for definite integrals can be seen in \url{https://en.wikipedia.org/wiki/Mean_value_theorem}.
	
	\end{solution}
	
	% 1.15
	\begin{exercise}
	As a further illustration of the consistency of our notation, consider the matrix representation of an operator $\mathscr{O}$ in the basis $\{\psi_i(x)\}$. Starting with
	\begin{equation}
		\mathscr{O} \psi_i(x) = \sum_{j} \psi_j(x) O_{ji} . \tag{1}
	\end{equation}
	Show that
	\begin{equation*}
		O_{ji} = \int \dif x \psi^*_j(x) \mathscr{O} \psi_i(x) .
	\end{equation*}
	Then using Eqs.(1.127a) and (1.138) rewrite (1) in the bra-ket notation and show that it is identical to Eq.(1.55).
	\end{exercise}
	
	\begin{solution}
	
	From (1), we find that
	\[
		\int \dif x \psi^*_j(x) \mathscr{O} \psi_i(x) = \int \dif x \psi^*_j(x) \sum_{k} \psi_k(x) O_{ki} = \sum_{k} \int \dif x \psi^*_j(x) \psi_k(x) O_{ki} = \sum_{k} \delta_{jk} O_{ki} = O_{ji} .
	\]
	Therefore, using (1.127a) and (1.138), (1.55) equals
	\[
		\mathscr{O} | i \rangle = \sum_j | j \rangle O_{ji} .
	\]
	
	\end{solution}	
	
	% 1.16
	\begin{exercise}
	Consider the eigenvalue problem
	\begin{equation}
		\mathcal{O} \phi(x) = \omega \phi(x).
	\end{equation}
	By expanding $\phi$ in the complete set $\{\psi_i(x),i=1,2,\cdots\}$ as
	\begin{equation*}
		\phi(x) = \sum_{i=1}^\infty c_i \psi_i(x)
	\end{equation*}
	show that it becomes equivalent to the matrix eigenvalue problem
	\begin{equation*}
		\Op {\bf c} = \omega {\bf c}
	\end{equation*}
	where $({\bf c})_i=c_i$ and $({\bf O})_{ij} = \int \dif x \psi^*_i(x)\mathcal{O}\psi_j(x)$. Do this with and without using bra-ket notation. Note that $\Op$ is an infinite matrix. In practice, we cannot handle infinite matrices. To keep things manageable, one uses only a finite subset of the set $\{\psi_i(x)\}$, i.e., $\{\psi_i(x),i=1,2,\cdots, N\}$. If the above analysis is repeated in this subspace, we obtain an $N \times N$ eigenvalue problem. As we shall see in Section 1.3, the corresponding $N$ eigenvalues approximate the true eigenvalues. In particular, we shall prove that the lowest eigenvalue of the truncated eigenvalue problem is greater or equal to the exact lowest eigenvalue.
	\end{exercise}
	
	\begin{solution}
		1-16 so
	\end{solution}
	
\end{document}